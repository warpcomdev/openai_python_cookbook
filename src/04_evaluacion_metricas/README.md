# Módulo 4: Evaluación y Métricas

Este módulo enseña cómo evaluar la calidad de las respuestas generadas por modelos LLM, tanto de forma automática como humana, y cómo comparar diferentes modelos o configuraciones.

## Contenido

- 00_introduccion.md: ¿Por qué y cómo evaluar modelos LLM?
- 01_metricas_automaticas.py: Ejemplo de evaluación automática con métricas como similitud de embeddings, BLEU, ROUGE, etc.
- 02_evaluacion_humana.md: Cómo diseñar una evaluación humana sencilla.
- 03_buenas_practicas.md: Consejos y limitaciones de las métricas automáticas y humanas.
- 04_recursos.md: Bibliotecas y enlaces útiles para evaluación.

## Requisitos

- Python 3.8+
- Paquetes: numpy, openai, nltk, scikit-learn, evaluate (HuggingFace)

## Recursos útiles

- [HuggingFace evaluate](https://huggingface.co/docs/evaluate/index)
- [NLTK](https://www.nltk.org/)
- [scikit-learn](https://scikit-learn.org/stable/) 